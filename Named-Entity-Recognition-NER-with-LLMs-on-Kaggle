{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10805354,"sourceType":"datasetVersion","datasetId":6706922},{"sourceId":256571,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":204044,"modelId":225262}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mehrdadal2023/named-entity-recognition-ner-llms-kaggle?scriptVersionId=223590899\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# ğŸ“Œ **Named Entity Recognition (NER) with Large-Scale LLMs & BERT-Based Chunking** ğŸ§ ğŸ“Š  \n\n## ğŸ” **Overview**\nThis Python script efficiently **extracts named entities** from large text documents using **DeepSeek-R1 Distill LLaMA-8B** as the LLM and **BERT (bert-base-uncased)** for **sentence-aware text chunking**. The extracted named entities are categorized into various types (e.g., **Person, Organization, Location, etc.**) and saved in a structured CSV file.  \n\n### ğŸ”¥ **Why is this useful?**  \nâœ… Enables **scalable named entity extraction** from **long documents**.  \nâœ… Uses **BERT for chunking**, ensuring **meaningful sentence segmentation**.  \nâœ… Employs a **Large-Scale LLM (DeepSeek-R1 Distill LLaMA-8B)** for **high-quality NER extraction**.  \nâœ… **Saves structured data** into a CSV file for further processing.\n\n---\n\n## ğŸ“Œ **Pipeline Overview** ğŸ”„  \n\n### **ğŸ“ Step 1: Read Input File**\nğŸ“‚ Reads the input text file containing **large amounts of unstructured text**.  \nğŸ“Œ **File path:** `/kaggle/input/ne-session1/NE.txt`  \n\n### **ğŸ§© Step 2: Chunking the Text using BERT**\nğŸ”¹ Long documents are **split into smaller chunks** (max 256 tokens) using **BERT tokenization**.  \nğŸ”¹ **Why BERT?** BERT ensures **meaningful** sentence-aware segmentation.  \n\n### **ğŸ›  Step 3: Named Entity Recognition (NER) using LLM**\nğŸ”¹ Each **text chunk** is fed into the **DeepSeek-R1 Distill LLaMA-8B** LLM.  \nğŸ“Œ **Model path:** `/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-llama-8b/2`  \nğŸ”¹ LLM **extracts named entities** along with their types (e.g., **Person, Organization, etc.**).  \nğŸ”¹ **Output follows this structured format:**\n```python\n[('Barack Obama', 'PERSON'), ('Google', 'ORGANIZATION'), ('New York', 'LOCATION')]\n","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BertTokenizerFast\n\n# Set environment variables\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# ğŸ“Œ Specify Model Names\nllm_model_name = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-llama-8b/2\"\nchunking_model_name = \"bert-base-uncased\"  # âœ… Use BERT for chunking\ncache_dir = \"/kaggle/temp\"  # âœ… Use Kaggle's temp storage\ninput_file_path = \"/kaggle/input/ne-session1/NE.txt\"  # âœ… Input file path\noutput_file_path = \"/kaggle/working/named_entities.xlsx\"  # âœ… Output file path\n\n# ğŸ”¥ Generation Parameters\nTEMPERATURE = 0.3  \nTOP_P = 0.95  \nMAX_LENGTH = 512  \n\n# âœ… Load Tokenizers & Models\nbert_tokenizer = BertTokenizerFast.from_pretrained(chunking_model_name)\nllm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name, cache_dir=cache_dir, trust_remote_code=True)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    llm_model_name, cache_dir=cache_dir, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n)\n\n# ğŸ”¹ Read Input File\ndef read_file(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\n# ğŸ”¹ Chunk Text Using BERT Tokenizer\ndef chunk_text_bert(text, max_tokens=256):\n    tokens = bert_tokenizer.encode(text, add_special_tokens=False)\n    chunks = [bert_tokenizer.decode(tokens[i : i + max_tokens], skip_special_tokens=True) for i in range(0, len(tokens), max_tokens)]\n    return chunks\n\n# ğŸ”¹ Generate Named Entities from LLM\ndef generate_named_entity_response(chunk):\n    prompt = f\"\"\"\n    Extract named entities and their types from the following text:\n    \n    {chunk}\n    \n    Return results as a structured list of entities. Each entity should be in the format:\n    (Entity: ENTITY_NAME, Type: ENTITY_TYPE)\n    \n    Only return the list, do not include explanations.\n    \"\"\"\n\n    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(\"cuda\")\n\n    with torch.no_grad():\n        outputs = llm_model.generate(\n            **inputs,\n            max_new_tokens=700,  \n            do_sample=True,  \n            temperature=TEMPERATURE,\n            top_p=TOP_P,\n            repetition_penalty=1.1\n        )\n\n    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\n    # ğŸ”¹ Print raw response for debugging\n    print(\"&&&&&&&&&&&&&&&&&\")\n\n    print(\"\\nğŸ”¹ Raw Model Response:\\n\", response)\n\n    print(\"&&&&&&&&&&&&&&&&&\")\n\n    # âœ… Extract entity-type pairs using regex\n    pattern = r\"\\(Entity:\\s*(.*?),\\s*Type:\\s*(.*?)\\)\"\n    entities = re.findall(pattern, response)\n    # âœ… Ensure extracted entities are stored correctly\n    processed_entities = [{\"Entity\": entity.strip(), \"Type\": entity_type.strip()} for entity, entity_type in entities]\n\n    return processed_entities if processed_entities else [{\"Entity\": \"N/A\", \"Type\": \"N/A\"}]  \n\n# ğŸ”¹ Extract Named Entities from Chunks\ndef extract_named_entities(chunks):\n    all_entities = []\n    \n    for chunk in chunks:\n        named_entities = generate_named_entity_response(chunk)\n\n        for entity_data in named_entities:  # âœ… Process each entity in the list\n            all_entities.append({\"Chunk\": chunk, \"Entity\": entity_data[\"Entity\"], \"Type\": entity_data[\"Type\"]})\n\n        # âœ… Debugging Output\n        print(\"\\n===========================\")\n        print(f\"ğŸ“Œ **Chunk**: {chunk}\")\n        print(f\"ğŸ” **Extracted Entities**: {named_entities}\")\n        print(\"===========================\")\n\n    return all_entities\n\n# âœ… Run the Process\ntext_data = read_file(input_file_path)\ntext_chunks = chunk_text_bert(text_data, max_tokens=256)\nextracted_data = extract_named_entities(text_chunks)\n\n# âœ… Convert to Pandas DataFrame & Save\ndf = pd.DataFrame(extracted_data)\nprint(\"\\nğŸ“Š **Final DataFrame Preview:**\")\nprint(df)  \ndf.to_excel(output_file_path, index=False)\n\nprint(f\"\\nâœ… Named entity extraction complete! Output saved to: {output_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T15:52:11.197889Z","iopub.execute_input":"2025-02-20T15:52:11.198203Z","iopub.status.idle":"2025-02-20T15:55:06.267512Z","shell.execute_reply.started":"2025-02-20T15:52:11.198178Z","shell.execute_reply":"2025-02-20T15:55:06.266685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}